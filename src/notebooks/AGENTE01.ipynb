{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13a9b43",
   "metadata": {},
   "source": [
    "# C√©lula 1: AGENTE01 ‚Äî Agente de IA para Perguntas e Respostas\n",
    "\n",
    "Este agente tem como objetivo responder a perguntas feitas pelo usu√°rio com base em um conte√∫do pr√©-carregado (como arquivos `.txt`, `.pdf` ou outros documentos). \n",
    "\n",
    "A ideia central √© criar uma interface simples e funcional para interagir com um LLM (Large Language Model), utilizando ferramentas como:\n",
    "\n",
    "- **LangChain**: para orquestra√ß√£o do agente e carregamento dos dados\n",
    "- **Modelos Hugging Face**: para gerar respostas com base nos dados\n",
    "- **Chroma**: para armazenar embeddings e fazer buscas sem√¢nticas\n",
    "\n",
    "---\n",
    "\n",
    "## Aplica√ß√µes poss√≠veis:\n",
    "- Consulta a manuais t√©cnicos\n",
    "- Resumo de relat√≥rios\n",
    "- Atendimento autom√°tico baseado em documentos\n",
    "\n",
    "---\n",
    "\n",
    "## Etapas abordadas neste notebook:\n",
    "\n",
    "**üîπ C√©lula 2 ‚Äì Defini√ß√£o do texto base**\n",
    "- Define manualmente o conte√∫do de conhecimento.\n",
    "- Utilizado como contexto para gera√ß√£o de embeddings e respostas.\n",
    "\n",
    "**üîπ C√©lula 3 ‚Äì Gera√ß√£o de embeddings com modelo local**\n",
    "- Cria um documento LangChain a partir do texto.\n",
    "- Divide o conte√∫do em partes com o `CharacterTextSplitter`.\n",
    "- Gera embeddings com modelo local `sentence-transformers/all-MiniLM-L6-v2`.\n",
    "- Armazena os embeddings em uma base Chroma para consultas futuras.\n",
    "\n",
    "**üîπ C√©lula 4 ‚Äì Consulta sem√¢ntica**\n",
    "- Recebe uma pergunta do usu√°rio via `input()`.\n",
    "- Recupera os documentos mais relevantes da base vetorial.\n",
    "- Exibe o conte√∫do mais semelhante para uso como contexto.\n",
    "\n",
    "**üîπ C√©lula 5 ‚Äì Simula√ß√£o automatizada**\n",
    "- Cont√©m uma pergunta fixa (sem input) para simular a consulta autom√°tica.\n",
    "- Realiza busca sem√¢ntica e exibe o conte√∫do retornado.\n",
    "\n",
    "**üîπ C√©lula 6 ‚Äì Execu√ß√£o do agente com modelo generativo (FLAN-T5)**\n",
    "- Usa `pipeline` com o modelo `google/flan-t5-base`.\n",
    "- Monta o prompt com o texto base e a pergunta.\n",
    "- Gera uma resposta textual com base no contexto.\n",
    "- OBS: O modelo √© treinado em ingl√™s, mas pode responder em portugu√™s.\n",
    "\n",
    "**üîπ C√©lula 7 ‚Äì Execu√ß√£o do agente com modelo generativo (mT5)**\n",
    "- Usa o modelo `google/mt5-small`, com suporte multil√≠ngue.\n",
    "- Gera respostas mais completas em portugu√™s, a partir do conte√∫do carregado.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e0aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execu√ß√£o interrompida da c√©lula...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 2: Bibliotecas principais do agente local\n",
    "    Esta c√©lula instala as bibliotecas necess√°rias para o agente local.\n",
    "\"\"\"\n",
    "\n",
    "executar_celula_5 = False\n",
    "\n",
    "if executar_celula_5:\n",
    "    # Bibliotecas principais do agente local\n",
    "    !pip install -U langchain langchain-community chromadb tiktoken sentence-transformers transformers\n",
    "    # Leitura de documentos (opcional)\n",
    "    !pip install unstructured pdfminer.six python-docx\n",
    "    # (Opcional) Interface interativa\n",
    "    !pip install ipywidgets\n",
    "    !pip install blobfile\n",
    "    !pip install sentencepiece\n",
    "    !pip install sacremoses\n",
    "\n",
    "else:\n",
    "    # c√≥digo da c√©lula 5\n",
    "    print(\"Execu√ß√£o interrompida da c√©lula...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_15416\\460049780.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 3: Carregamento de conte√∫do fixo e cria√ß√£o dos embeddings locais\n",
    "\n",
    "Esta c√©lula:\n",
    "- Define um texto manual como base de conhecimento\n",
    "- Separa o texto em pequenos blocos (chunking)\n",
    "- Gera embeddings com modelo local HuggingFace (sentence-transformers)\n",
    "- Armazena os embeddings em uma base vetorial local usando Chroma\n",
    "\"\"\"\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# Texto fixo como exemplo de base de conhecimento\n",
    "texto = \"\"\"\n",
    "A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ß√£o que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n",
    "Essas tarefas incluem reconhecimento de fala, tomada de decis√µes, tradu√ß√£o de idiomas e muito mais.\n",
    "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar infer√™ncias e responder a comandos.\n",
    "\"\"\"\n",
    "\n",
    "# Separar o texto em blocos menores para melhor indexa√ß√£o\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "documentos = text_splitter.split_documents([Document(page_content=texto)])\n",
    "\n",
    "# Gerar embeddings locais com modelo Hugging Face\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Criar e armazenar os vetores em uma base vetorial local com Chroma\n",
    "vectorstore = Chroma.from_documents(documentos, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396ce776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Que tarefas a IA pode executar, segundo o texto?\n",
      "Resposta: normalmente exigiriam intelig√™ncia humana\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 4: Execu√ß√£o do agente de perguntas e respostas\n",
    "\n",
    "Esta c√©lula:\n",
    "- Recebe uma pergunta do usu√°rio\n",
    "- Usa o modelo local 'deepset/roberta-base-squad2' via Transformers\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, roberta-base-squad2, RESPOSTAS GENERICAS\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline de QA\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Texto base como contexto\n",
    "contexto = texto\n",
    "\n",
    "# # Intera√ß√£o\n",
    "# pergunta = input(\"Digite sua pergunta: \")\n",
    "# resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "\n",
    "# Pergunta simulada\n",
    "pergunta = \"Que tarefas a IA pode executar, segundo o texto?\"\n",
    "resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a71701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Que tarefas a IA pode executar, segundo o texto?\n",
      "Resposta: normalmente exigiriam intelig√™ncia humana\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 5: Execu√ß√£o do agente de perguntas e respostas\n",
    "\n",
    "Esta c√©lula:\n",
    "- Recebe uma pergunta do usu√°rio\n",
    "- Usa o modelo local 'distilbert-base-cased-distilled-squad' via Transformers\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, model=\"distilbert-base-cased-distilled-squad RESPOSTAS GENERICAS\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Modelo alternativo mais objetivo\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Texto de contexto da c√©lula 3\n",
    "contexto = texto\n",
    "\n",
    "# Pergunta simulada\n",
    "pergunta = \"Que tarefas a IA pode executar, segundo o texto?\"\n",
    "\n",
    "# Gera√ß√£o da resposta\n",
    "resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f31d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Liste as tarefas que a intelig√™ncia artificial pode executar, segundo o texto: \n",
      "A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ß√£o que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n",
      "Essas tarefas incluem reconhecimento de fala, tomada de decis√µes, tradu√ß√£o de idiomas e muito mais.\n",
      "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar infer√™ncias e responder a comandos.\n",
      "\n",
      "Resposta: List as tasks that artificial intelligence (AI) can execute, secondo o texto: Artificial intelligence (AI) √© o ramo da ci√™ncia da computa√ßo que se dedica a criar sistemas capazes de executar tasks that normally require human intelligence. These tasks include reconhecimento de fala, tomada decises, traducio de idiomas e muito mais. The agents of AI use three-dimensional models in large quantities of data to perform inferences and respond to commands.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 6: Execu√ß√£o do agente de perguntas e respostas\n",
    "\n",
    "Esta c√©lula:\n",
    "- Recebe uma pergunta do usu√°rio\n",
    "- usar modelo generativo para perguntas e respostas\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, model=google/flan-t5-base RESPOSTAS MAIS COMPLETAS\n",
    "- Usa o modelo google/flan-t5-base para gerar respostas completas com base no contexto.\n",
    "\n",
    "Obs.: Treinado em ingl√™s, mas pode responder em portugu√™s\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline de gera√ß√£o de texto com modelo T5\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Montar prompt com contexto\n",
    "prompt = f\"Liste as tarefas que a intelig√™ncia artificial pode executar, segundo o texto: {texto}\"\n",
    "\n",
    "# Gerar resposta\n",
    "resposta = qa_pipeline(prompt, max_length=60, do_sample=False)\n",
    "\n",
    "print(\"Pergunta:\", prompt)\n",
    "print(\"Resposta:\", resposta[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46680b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Pergunta: perguntar: O que √© IA?\n",
      "\n",
      "A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ß√£o que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n",
      "Essas tarefas incluem reconhecimento de fala, tomada de decis√µes, tradu√ß√£o de idiomas e muito mais.\n",
      "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar infer√™ncias e responder a comandos.\n",
      "\n",
      "Resposta: <extra_id_0>.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 7: Execu√ß√£o do agente com modelo generativo (google/mt5-small)\n",
    "\n",
    "Esta c√©lula:\n",
    "- Recebe uma pergunta do usu√°rio como prompt\n",
    "- Utiliza um modelo generativo para responder com base no texto carregado\n",
    "- Emprega o modelo local 'google/mt5-small', que oferece suporte multil√≠ngue, incluindo o portugu√™s\n",
    "- Gera respostas mais completas e contextuais com base no conte√∫do fornecido\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\", use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Reformulado com o prefixo \"perguntar:\", compat√≠vel com o estilo MT5\n",
    "prompt = f\"perguntar: Quais tarefas a intelig√™ncia artificial pode executar de acordo com o texto abaixo?\\n{texto}\"\n",
    "prompt = f\"perguntar: O que √© IA?\\n{texto}\"\n",
    "# prompt = f\"perguntar: Qual √© a previs√£o do tempo para amanh√£ em S√£o Paulo?\\n{texto}\"\n",
    "\n",
    "# Lista de perguntas para testar o modelo\n",
    "resposta = qa_pipeline(prompt, max_new_tokens=60, do_sample=False)\n",
    "\n",
    "print(type(resposta))\n",
    "print(\"Pergunta:\", prompt)\n",
    "print(\"Resposta:\", resposta[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ab86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: definicao\n",
      "Pergunta: Explique o conceito de IA.\n",
      "Resposta: \"\" A intelig√™ncia artificial (IA) √© uma √°rea da ci√™ncia da computa√ßo que desenvolve sistemas capazes de simular intelig√™ncia humana. These systems aprendem com datos, identificam padres, tomam decises e resolvem problemas aut√¥noma ou assistida. Entre as tarefas comuns de IA esto: reconhecimento de fala, traducio autom√°tica, an√°lise de imagens, auto\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 7: Execu√ß√£o do agente com modelo generativo (google/flan-t5-base)\n",
    "\n",
    "Melhorias aplicadas:\n",
    "- Texto base expandido\n",
    "- Prompt reformulado com instru√ß√µes claras\n",
    "- Aumento do n√∫mero de tokens para evitar corte da resposta\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Texto expandido\n",
    "texto = \"\"\"\n",
    "A intelig√™ncia artificial (IA) √© uma √°rea da ci√™ncia da computa√ß√£o que desenvolve sistemas capazes de simular a intelig√™ncia humana.\n",
    "Esses sistemas aprendem com dados, identificam padr√µes, tomam decis√µes e resolvem problemas de maneira aut√¥noma ou assistida.\n",
    "Entre as tarefas comuns da IA est√£o: reconhecimento de fala, tradu√ß√£o autom√°tica, an√°lise de imagens, automa√ß√£o de processos,\n",
    "assistentes virtuais e recomenda√ß√£o personalizada de conte√∫dos. A IA est√° presente em √°reas como sa√∫de, finan√ßas, educa√ß√£o,\n",
    "log√≠stica e atendimento ao cliente.\n",
    "\"\"\"\n",
    "\n",
    "# Dicion√°rio de perguntas por categoria\n",
    "perguntas = {\n",
    "    \"definicao\": [\n",
    "        \"O que √© intelig√™ncia artificial?\",\n",
    "        \"Explique o conceito de IA.\",\n",
    "        \"O que significa IA?\"\n",
    "    ],\n",
    "    \"tarefas\": [\n",
    "        \"Quais tarefas a intelig√™ncia artificial pode executar?\",\n",
    "        \"A IA pode reconhecer fala, traduzir idiomas e tomar decis√µes?\",\n",
    "        \"Liste exemplos de atividades feitas por IA.\"\n",
    "    ],\n",
    "    \"limites\": [\n",
    "        \"Quais s√£o as limita√ß√µes da IA?\",\n",
    "        \"A IA pode substituir completamente os humanos?\",\n",
    "        \"A intelig√™ncia artificial pode tomar decis√µes sozinha?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Escolha aleat√≥ria\n",
    "categoria = random.choice(list(perguntas.keys()))\n",
    "pergunta = random.choice(perguntas[categoria])\n",
    "\n",
    "# Prompt aprimorado\n",
    "prompt = f\"\"\"\n",
    "Com base no texto a seguir, responda √† pergunta de forma clara, completa e original.\n",
    "Texto:\n",
    "\\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "\n",
    "Pergunta:\n",
    "{pergunta}\n",
    "\n",
    "Responda:\n",
    "\"\"\"\n",
    "\n",
    "# Modelo e pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Gera√ß√£o\n",
    "resposta = qa_pipeline(prompt, max_new_tokens=150, do_sample=False)\n",
    "\n",
    "# Exibir\n",
    "print(\"Categoria:\", categoria)\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0850753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Categoria: definicao\n",
      "Pergunta: O que significa IA?\n",
      "Resposta: A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ßo que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C√©lula 7: Execu√ß√£o do agente com modelo generativo (google/flan-t5-large)\n",
    "\n",
    "Esta c√©lula:\n",
    "- Define um dicion√°rio com perguntas agrupadas por categorias\n",
    "- Seleciona aleatoriamente uma pergunta\n",
    "- Utiliza o modelo local 'google/flan-t5-large', que oferece maior capacidade de compreens√£o\n",
    "- Gera uma resposta com base no conte√∫do carregado como texto base\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Texto fixo como base de conhecimento\n",
    "texto = \"\"\"\n",
    "A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ß√£o que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n",
    "Essas tarefas incluem reconhecimento de fala, tomada de decis√µes, tradu√ß√£o de idiomas e muito mais.\n",
    "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar infer√™ncias e responder a comandos.\n",
    "\"\"\"\n",
    "\n",
    "# Dicion√°rio de perguntas agrupadas por categorias\n",
    "perguntas = {\n",
    "    \"definicao\": [\n",
    "        \"O que √© intelig√™ncia artificial?\",\n",
    "        \"Explique o conceito de IA.\",\n",
    "        \"O que significa IA?\"\n",
    "    ],\n",
    "    \"tarefas\": [\n",
    "        \"Quais tarefas a intelig√™ncia artificial pode executar?\",\n",
    "        \"A IA pode reconhecer fala, traduzir idiomas e tomar decis√µes?\",\n",
    "        \"Liste exemplos de atividades feitas por IA.\"\n",
    "    ],\n",
    "    \"limites\": [\n",
    "        \"Quais s√£o as limita√ß√µes da IA?\",\n",
    "        \"A IA pode substituir completamente os humanos?\",\n",
    "        \"A intelig√™ncia artificial pode tomar decis√µes sozinha?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Selecionar uma pergunta aleat√≥ria\n",
    "categoria = random.choice(list(perguntas.keys()))\n",
    "pergunta = random.choice(perguntas[categoria])\n",
    "\n",
    "# Reformular o prompt para melhor compreens√£o\n",
    "prompt = f\"Responda em portugu√™s com detalhes: {pergunta}\\n\\nBase de conhecimento:\\n{texto}\"\n",
    "\n",
    "# Carregar modelo e tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Gerar resposta com mais tokens\n",
    "resposta = qa_pipeline(prompt, max_new_tokens=150, do_sample=False)\n",
    "\n",
    "# Exibir resultado\n",
    "print(\"Device set to use cpu\")\n",
    "print(\"Categoria:\", categoria)\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta[0]['generated_text'].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994c775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria: tarefas\n",
      "Pergunta: A IA pode reconhecer fala, traduzir idiomas e tomar decis√µes?\n",
      "Resposta: A intelig√™ncia artificial (IA) √© o n√∫cleo da ci√™ncia da computa√ß√£o que permite que os sistemas executem tarefas que normalmente exigem intelig√™ncia humana. Essas tarefas incluem o reconhecimento da fala, a tomada de decis√µes, a tradu√ß√£o de sons e muito mais.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- Texto fixo como base de conhecimento ---\n",
    "texto_pt = \"\"\"\n",
    "A intelig√™ncia artificial (IA) √© o ramo da ci√™ncia da computa√ß√£o que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam intelig√™ncia humana.\n",
    "Essas tarefas incluem reconhecimento de fala, tomada de decis√µes, tradu√ß√£o de idiomas e muito mais.\n",
    "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar infer√™ncias e responder a comandos.\n",
    "\"\"\"\n",
    "\n",
    "# --- Dicion√°rio de perguntas ---\n",
    "perguntas = {\n",
    "    \"definicao\": [\n",
    "        \"O que √© intelig√™ncia artificial?\",\n",
    "        \"Explique o conceito de IA.\",\n",
    "        \"O que significa IA?\"\n",
    "    ],\n",
    "    \"tarefas\": [\n",
    "        \"Quais tarefas a intelig√™ncia artificial pode executar?\",\n",
    "        \"A IA pode reconhecer fala, traduzir idiomas e tomar decis√µes?\",\n",
    "        \"Liste exemplos de atividades feitas por IA.\"\n",
    "    ],\n",
    "    \"limites\": [\n",
    "        \"Quais s√£o as limita√ß√µes da IA?\",\n",
    "        \"A IA pode substituir completamente os humanos?\",\n",
    "        \"A intelig√™ncia artificial pode tomar decis√µes sozinha?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Sele√ß√£o aleat√≥ria de pergunta ---\n",
    "categoria = random.choice(list(perguntas.keys()))\n",
    "pergunta_pt = random.choice(perguntas[categoria])\n",
    "\n",
    "# --- Pipelines de tradu√ß√£o ---\n",
    "# Instale opcionalmente: pip install sacremoses\n",
    "tradutor_pt_en = pipeline(\"translation\", model=\"manueldeprada/t5-small-pt-en\")\n",
    "tradutor_en_pt = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")\n",
    "\n",
    "# Tradu√ß√£o da pergunta e texto\n",
    "pergunta_en = tradutor_pt_en(pergunta_pt)[0][\"translation_text\"]\n",
    "texto_en = tradutor_pt_en(texto_pt)[0][\"translation_text\"]\n",
    "\n",
    "# --- Prompt para resposta ---\n",
    "prompt = f\"Question: {pergunta_en}\\n\\nContext:\\n{texto_en}\\n\\nAnswer in English:\"\n",
    "\n",
    "# --- Carregando modelo de QA (Flan-T5) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Gera√ß√£o da resposta (em ingl√™s)\n",
    "resposta_en = qa_pipeline(prompt, max_new_tokens=150, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "# Tradu√ß√£o da resposta para portugu√™s\n",
    "resposta_pt = tradutor_en_pt(resposta_en)[0][\"translation_text\"]\n",
    "\n",
    "# --- Exibi√ß√£o do resultado final ---\n",
    "print(\"Categoria:\", categoria)\n",
    "print(\"Pergunta:\", pergunta_pt)\n",
    "print(\"Resposta:\", resposta_pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf6e8a",
   "metadata": {},
   "source": [
    "### üìé Anexo: Gloss√°rio de Termos T√©cnicos\n",
    "\n",
    "Este gloss√°rio tem como objetivo auxiliar na compreens√£o dos principais conceitos utilizados no desenvolvimento do agente de IA presente neste notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Embeddings\n",
    "Embeddings s√£o representa√ß√µes vetoriais de textos, palavras ou senten√ßas em um espa√ßo num√©rico de alta dimens√£o. Eles capturam o significado sem√¢ntico dos dados e permitem comparar similaridades entre conte√∫dos diferentes. S√£o fundamentais para buscas por similaridade em bases vetoriais.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Vector Store\n",
    "√â uma base de dados especializada em armazenar vetores (como embeddings). Permite realizar buscas sem√¢nticas ao encontrar vetores semelhantes a um vetor de consulta. No projeto, o Chroma √© utilizado como vector store.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Prompt\n",
    "Prompt √© o texto de entrada enviado ao modelo de linguagem (LLM) para gerar uma resposta. Um bom prompt orienta o modelo a produzir respostas mais relevantes e espec√≠ficas.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Pipeline (HuggingFace)\n",
    "Um pipeline √© uma abstra√ß√£o que simplifica o uso de modelos da HuggingFace. Ele automatiza tarefas como tokeniza√ß√£o, entrada e sa√≠da de dados e infer√™ncia do modelo. Exemplo: `pipeline(\"text2text-generation\")` para gerar texto baseado em outro texto.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ LLM (Large Language Model)\n",
    "Modelos de linguagem de grande porte treinados em vastos conjuntos de dados. S√£o capazes de compreender e gerar linguagem humana de forma sofisticada. Exemplos: GPT, BERT, T5.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
