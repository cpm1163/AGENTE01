{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13a9b43",
   "metadata": {},
   "source": [
    "# AGENTE01 — Agente de IA para Perguntas e Respostas\n",
    "\n",
    "Este agente tem como objetivo responder a perguntas feitas pelo usuário com base em um conteúdo pré-carregado (como arquivos `.txt`, `.pdf` ou outros documentos). \n",
    "\n",
    "A ideia central é criar uma interface simples e funcional para interagir com um LLM (Large Language Model), utilizando ferramentas como:\n",
    "\n",
    "- **LangChain**: para orquestração do agente e carregamento dos dados\n",
    "- **OpenAI API**: para gerar respostas com base nos dados\n",
    "- **Chroma** ou outro vector store: para armazenar embeddings e fazer buscas semânticas\n",
    "\n",
    "**Aplicações possíveis**:\n",
    "- Consulta a manuais técnicos\n",
    "- Resumo de relatórios\n",
    "- Atendimento automático baseado em documentos\n",
    "\n",
    "---\n",
    "\n",
    "**Etapas que serão abordadas neste notebook**:\n",
    "1. Instalação das bibliotecas\n",
    "2. Carregamento dos documentos\n",
    "3. Criação do index vetorial (embeddings)\n",
    "4. Configuração do agente com LangChain\n",
    "5. Execução de perguntas interativas\n",
    "\n",
    "\n",
    "**Instalação das bibliotecas principais**;\n",
    "1. pip install openai langchain chromadb tiktoken wheel\n",
    "\n",
    "# Leitura de documentos (.txt, .pdf, .docx etc)\n",
    "2. pip install unstructured pdfminer.six python-docx\n",
    "\n",
    "# (Opcional) Biblioteca para carregar arquivos via widgets\n",
    "3. pip install ipywidgets\n",
    "\n",
    "# Se estiver usando dotenv para chaves (recomendado)\n",
    "4. pip install python-dotenv\n",
    "\n",
    "Obs.: A instalação foi concluída com sucesso, e esse aviso do langdetect é apenas uma mensagem de depreciação — não impede o funcionamento do agente.\n",
    "\n",
    "**Carregamento de conteúdo fixo e criação dos embeddings**;\n",
    "Essa célula:\n",
    "\n",
    "Define um texto manual como base de conhecimento\n",
    "\n",
    "Cria um documento no formato utilizado pelo LangChain\n",
    "\n",
    "Gera os embeddings utilizando o modelo local sentence-transformers\n",
    "\n",
    "Armazena os embeddings em uma base vetorial com o Chroma para consultas futuras\n",
    "\n",
    "**Como ocultar o aviso (opcional)**;\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630e0aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução interrompida da célula...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 2: Bibliotecas principais do agente local\n",
    "    Esta célula instala as bibliotecas necessárias para o agente local.\n",
    "\"\"\"\n",
    "\n",
    "executar_celula_5 = False\n",
    "\n",
    "if executar_celula_5:\n",
    "    # Bibliotecas principais do agente local\n",
    "    !pip install -U langchain langchain-community chromadb tiktoken sentence-transformers transformers\n",
    "    # Leitura de documentos (opcional)\n",
    "    !pip install unstructured pdfminer.six python-docx\n",
    "    # (Opcional) Interface interativa\n",
    "    !pip install ipywidgets\n",
    "    !pip install blobfile\n",
    "    !pip install sentencepiece\n",
    "\n",
    "else:\n",
    "    # código da célula 5\n",
    "    print(\"Execução interrompida da célula...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_12196\\460049780.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 3: Carregamento de conteúdo fixo e criação dos embeddings locais\n",
    "\n",
    "Esta célula:\n",
    "- Define um texto manual como base de conhecimento\n",
    "- Separa o texto em pequenos blocos (chunking)\n",
    "- Gera embeddings com modelo local HuggingFace (sentence-transformers)\n",
    "- Armazena os embeddings em uma base vetorial local usando Chroma\n",
    "\"\"\"\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# Texto fixo como exemplo de base de conhecimento\n",
    "texto = \"\"\"\n",
    "A inteligência artificial (IA) é o ramo da ciência da computação que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam inteligência humana.\n",
    "Essas tarefas incluem reconhecimento de fala, tomada de decisões, tradução de idiomas e muito mais.\n",
    "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar inferências e responder a comandos.\n",
    "\"\"\"\n",
    "\n",
    "# Separar o texto em blocos menores para melhor indexação\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "documentos = text_splitter.split_documents([Document(page_content=texto)])\n",
    "\n",
    "# Gerar embeddings locais com modelo Hugging Face\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Criar e armazenar os vetores em uma base vetorial local com Chroma\n",
    "vectorstore = Chroma.from_documents(documentos, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ce776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Quais são os principais riscos éticos associados ao uso da inteligência artificial?\n",
      "Resposta: muito mais\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 4: Execução do agente de perguntas e respostas\n",
    "\n",
    "Esta célula:\n",
    "- Recebe uma pergunta do usuário\n",
    "- Usa o modelo local 'deepset/roberta-base-squad2' via Transformers\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, roberta-base-squad2, RESPOSTAS GENERICAS\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline de QA\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Texto base como contexto\n",
    "contexto = texto\n",
    "\n",
    "# # Interação\n",
    "# pergunta = input(\"Digite sua pergunta: \")\n",
    "# resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "\n",
    "# Pergunta simulada\n",
    "pergunta = \"Que tarefas a IA pode executar, segundo o texto?\"\n",
    "resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a71701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Que tarefas a IA pode executar, segundo o texto?\n",
      "Resposta: normalmente exigiriam inteligência humana\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 5: Execução do agente de perguntas e respostas\n",
    "\n",
    "Esta célula:\n",
    "- Recebe uma pergunta do usuário\n",
    "- Usa o modelo local 'distilbert-base-cased-distilled-squad' via Transformers\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, model=\"distilbert-base-cased-distilled-squad RESPOSTAS GENERICAS\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Modelo alternativo mais objetivo\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Texto de contexto da célula 3\n",
    "contexto = texto\n",
    "\n",
    "# Pergunta simulada\n",
    "pergunta = \"Que tarefas a IA pode executar, segundo o texto?\"\n",
    "\n",
    "# Geração da resposta\n",
    "resposta = qa_pipeline(question=pergunta, context=contexto)\n",
    "\n",
    "print(\"Pergunta:\", pergunta)\n",
    "print(\"Resposta:\", resposta['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f31d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Liste as tarefas que a inteligência artificial pode executar, segundo o texto: \n",
      "A inteligência artificial (IA) é o ramo da ciência da computação que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam inteligência humana.\n",
      "Essas tarefas incluem reconhecimento de fala, tomada de decisões, tradução de idiomas e muito mais.\n",
      "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar inferências e responder a comandos.\n",
      "\n",
      "Resposta: List as tasks that artificial intelligence (AI) can execute, secondo o texto: Artificial intelligence (AI) é o ramo da ciência da computaço que se dedica a criar sistemas capazes de executar tasks that normally require human intelligence. These tasks include reconhecimento de fala, tomada decises, traducio de idiomas e muito mais. The agents of AI use three-dimensional models in large quantities of data to perform inferences and respond to commands.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 6: Execução do agente de perguntas e respostas\n",
    "\n",
    "Esta célula:\n",
    "- Recebe uma pergunta do usuário\n",
    "- usar modelo generativo para perguntas e respostas\n",
    "- Retorna a resposta com base no texto carregado\n",
    "- Modelo com resposta a perguntas (QA) local, model=google/flan-t5-base RESPOSTAS MAIS COMPLETAS\n",
    "- Usa o modelo google/flan-t5-base para gerar respostas completas com base no contexto.\n",
    "\n",
    "Obs.: Treinado em inglês, mas pode responder em português\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline de geração de texto com modelo T5\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Montar prompt com contexto\n",
    "prompt = f\"Liste as tarefas que a inteligência artificial pode executar, segundo o texto: {texto}\"\n",
    "\n",
    "# Gerar resposta\n",
    "resposta = qa_pipeline(prompt, max_length=60, do_sample=False)\n",
    "\n",
    "print(\"Pergunta:\", prompt)\n",
    "print(\"Resposta:\", resposta[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46680b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Pergunta: perguntar: Quais tarefas a inteligência artificial pode executar de acordo com o texto abaixo?\n",
      "\n",
      "A inteligência artificial (IA) é o ramo da ciência da computação que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam inteligência humana.\n",
      "Essas tarefas incluem reconhecimento de fala, tomada de decisões, tradução de idiomas e muito mais.\n",
      "Os agentes de IA utilizam modelos treinados em grandes quantidades de dados para realizar inferências e responder a comandos.\n",
      "\n",
      "Resposta: <extra_id_0>.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Célula 7: Execução do agente com modelo generativo (google/mt5-small)\n",
    "\n",
    "Esta célula:\n",
    "- Recebe uma pergunta do usuário como prompt\n",
    "- Utiliza um modelo generativo para responder com base no texto carregado\n",
    "- Emprega o modelo local 'google/mt5-small', que oferece suporte multilíngue, incluindo o português\n",
    "- Gera respostas mais completas e contextuais com base no conteúdo fornecido\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\", use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Reformulado com o prefixo \"perguntar:\", compatível com o estilo MT5\n",
    "prompt = f\"perguntar: Quais tarefas a inteligência artificial pode executar de acordo com o texto abaixo?\\n{texto}\"\n",
    "# prompt = f\"perguntar: Qual é a previsão do tempo para amanhã em São Paulo?\\n{texto}\"\n",
    "\n",
    "# Lista de perguntas para testar o modelo\n",
    "resposta = qa_pipeline(prompt, max_new_tokens=60, do_sample=False)\n",
    "\n",
    "print(type(resposta))\n",
    "print(\"Pergunta:\", prompt)\n",
    "print(\"Resposta:\", resposta[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
